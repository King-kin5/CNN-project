{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0abea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9108503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cd173b7470>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.1\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07214a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a7b3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bff22dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98612ff6",
   "metadata": {},
   "source": [
    "This means we have 1000 examples of 28x28 pixels in grayscale (i.e. no rgb channels, hence the one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d07d188c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e3339",
   "metadata": {},
   "source": [
    "## Building the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0717f",
   "metadata": {},
   "source": [
    "Now let's go ahead and build our network. We'll use two 2-D convolutional layers followed by two fully-connected (or linear) layers. As activation function we'll choose rectified linear units (ReLUs in short) and as a means of regularization we'll use two dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5d47b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6948d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,10,kernel_size=5)\n",
    "        self.conv2=nn.Conv2d(10,20,kernel_size=5)\n",
    "        self.conv2_drop=nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de75ddd",
   "metadata": {},
   "source": [
    "Broadly speaking we can think of the torch.nn layers as which contain trainable parameters while torch.nn.functional are purely functional. The forward() pass defines the way we compute our output using the given layers and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "146a7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's initialize the network and the optimizer\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b9552",
   "metadata": {},
   "source": [
    "Note: If we were using a GPU for training, we should have also sent the network parameters to the GPU using e.g. network.cuda(). It is important to transfer the network's parameters to the appropriate device before passing them to the optimizer, otherwise the optimizer will not be able to keep track of them in the right way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1831910",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9548961",
   "metadata": {},
   "source": [
    "Time to build our training loop. First we want to make sure our network is in training mode. Then we iterate over all training data once per epoch. Loading the individual batches is handled by the DataLoader. First we need to manually set the gradients to zero using optimizer.zero_grad() since PyTorch by default accumulates gradients. We then produce the output of our network (forward pass) and compute a negative log-likelihodd loss between the output and the ground truth label. The backward() call we now collect a new set of gradients which we propagate back into each of the network's parameters using optimizer.step()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92f7c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3d74db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item()\n",
    "                )\n",
    "            )\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx * 64) + ((epoch - 1) * len(train_loader.dataset))\n",
    "            )\n",
    "\n",
    "            # âœ… create and save correctly\n",
    "            os.makedirs('results', exist_ok=True)\n",
    "            torch.save(network.state_dict(), 'results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), 'results/optimizer.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a20ac7",
   "metadata": {},
   "source": [
    "Neural network modules as well as optimizers have the ability to save and load their internal state using .state_dict(). With this we can continue training from previously saved state dicts if needed - we'd just need to call .load_state_dict(state_dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7a0bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d5338",
   "metadata": {},
   "source": [
    "Using the context manager no_grad() we can avoid storing the computations done producing the output of our network in the computation graph.\n",
    "\n",
    "Time to run the training! We'll manually add a test() call before we loop over n_epochs to evaluate our model with randomly initialized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfb3c452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adio\\AppData\\Local\\Temp\\ipykernel_8932\\461288150.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3096, Accuracy: 924/10000 (9%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.325928\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.181804\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.733285\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.313725\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.026400\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.007130\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.036813\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.809312\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.867944\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.693378\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.610975\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.561613\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.640599\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.520340\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.577988\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.868777\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.619951\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.688882\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.867956\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.487128\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.743566\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.528925\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.592478\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.420429\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.365159\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.736964\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.416748\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.350860\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.499933\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.408407\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.751143\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.454786\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.386571\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.420976\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.356504\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.479941\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.271528\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.163304\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.378802\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.470682\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.353530\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.177895\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.324782\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.536932\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.517411\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.296775\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.386951\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.361561\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.346417\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.294634\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.433913\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.321398\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.196264\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.330616\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.409687\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.304547\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.338274\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.262239\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.434562\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.565011\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.457188\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.618438\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.412269\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.333816\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.231545\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.185069\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.270741\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.372666\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.319358\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.366543\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.319210\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.162421\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.351536\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.271460\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.386251\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.262441\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.298288\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.239793\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.203716\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.308608\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.597413\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.247851\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.546946\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.471829\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.319671\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.264541\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.363132\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.133345\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.245503\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.190415\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.322127\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.114691\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.565108\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.525364\n",
      "\n",
      "Test set: Avg. loss: 0.1029, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.200153\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.308784\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.211723\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.282951\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.290581\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.234619\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.195268\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.341440\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.514038\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.528722\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.344277\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.372859\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.386097\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.367632\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.120235\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.255527\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.500551\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.378806\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.306894\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.255131\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.263043\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.258855\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.471525\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.118395\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.169465\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.289902\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.518171\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.116260\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.180247\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.207230\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.086774\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.235891\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.128408\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.332655\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.365418\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.218395\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.196241\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.244493\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.503270\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.326365\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.374387\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.196420\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.296038\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.185030\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.381479\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.289735\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.381087\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.125418\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.232591\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.238832\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.405994\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.324389\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.192502\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.271476\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.218413\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.348483\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.100545\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.137121\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.343048\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.183334\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.212789\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.355849\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.276159\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.350377\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.523204\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.248372\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.375085\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.281383\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.318251\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.273468\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.358171\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.279458\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.294493\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.249254\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.367280\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.115596\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.088468\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.202480\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.181799\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.272589\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.291102\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.350075\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.413261\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.266740\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.339268\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.320109\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.467416\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.245072\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.197511\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.233081\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.413085\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.087681\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.138981\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.462661\n",
      "\n",
      "Test set: Avg. loss: 0.0703, Accuracy: 9774/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.291743\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.189254\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.174412\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.098408\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.399963\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.215988\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.226955\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.466864\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.327630\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.338920\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.217309\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.319244\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.148705\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.081380\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.319669\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.373802\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.079522\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.234378\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.307945\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.317015\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.281897\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.550350\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.423176\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.100495\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.222223\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.163389\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.161388\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.207124\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.190158\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.113459\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.213695\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.177671\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.158186\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.279167\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.198136\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.179746\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.197633\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.133325\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.233914\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.160675\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.522526\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.178124\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.357331\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.326888\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.431448\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.218610\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.365016\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.231159\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.140589\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.318889\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.261757\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.119724\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.046255\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.264189\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.175796\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.258075\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.423694\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.157439\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.427212\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.158276\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.355151\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.375587\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.585100\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.295881\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.395008\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.191736\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.285470\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.313284\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.141613\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.243931\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.336977\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.361962\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.100750\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.192375\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.213569\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.299489\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.258852\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.380672\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.184757\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.459549\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.110478\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.354061\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.197952\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.069650\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.269855\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.353688\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.221356\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.228082\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.165101\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.260433\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.099977\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.144863\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.263747\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.250550\n",
      "\n",
      "Test set: Avg. loss: 0.0704, Accuracy: 9800/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.137994\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.081884\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.112650\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.148407\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.315911\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.128511\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.304542\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.196075\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.126374\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.075577\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.182328\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.171983\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.435392\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.318826\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.154003\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.165615\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.156488\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.636845\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.249994\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.203746\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.212666\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.051952\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.113717\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.291196\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.074236\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.114251\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.079562\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.401212\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.166752\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.123406\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.162532\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.146643\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.047117\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.156306\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.125254\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.189650\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.090990\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.185352\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.281269\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.411343\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.235600\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.319397\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.274412\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.142736\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.342248\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.187593\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.181930\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.173829\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.298674\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.177367\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.203996\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.100357\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.241919\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.114523\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.210797\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.341777\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.328698\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.255586\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.197695\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.168247\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.196543\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.241327\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.267323\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.135707\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.241924\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.234091\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.176476\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.221628\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.165705\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.237672\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.113022\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.184086\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.188063\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.153280\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.134219\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.045283\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.223917\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.109649\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.321305\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.415313\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.116337\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.114221\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.216314\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.111420\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.300412\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.177731\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.446170\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.452110\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.147700\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.309858\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.307270\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.192808\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.295923\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.397955\n",
      "\n",
      "Test set: Avg. loss: 0.0599, Accuracy: 9817/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.130440\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.073358\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.122057\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.287200\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.246820\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.081396\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.343103\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.114911\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.266072\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.209018\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.126501\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.407070\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.080239\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.173741\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.158273\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.173227\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.183891\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.209495\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.103375\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.193052\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.087702\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.350963\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.257706\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.148183\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.164511\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.193577\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.271137\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.277461\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.138444\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.342323\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.319999\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.188480\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.233579\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.173387\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.283993\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.244397\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.253109\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.202294\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.220136\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.136267\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.094745\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.267550\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.092726\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.399465\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.094866\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.158672\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.216993\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.382336\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.125526\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.151463\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.316397\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.271231\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.135216\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.103669\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.214537\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.119170\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.188786\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.212265\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.395838\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.168708\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.160754\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.081538\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.095883\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.166068\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.236201\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.359018\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.066528\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.127847\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.281699\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.320693\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.174905\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.173117\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.170670\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.157135\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.462165\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.054914\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.415401\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.313578\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.225507\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.076623\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.146771\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.161850\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.264631\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.084800\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.374962\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.184774\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.157784\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.296107\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.257885\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.105471\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.172825\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.192590\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.176624\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.137177\n",
      "\n",
      "Test set: Avg. loss: 0.0680, Accuracy: 9779/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.202561\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.145460\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.170883\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.142331\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.063434\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.159231\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.148894\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.243253\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.301878\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.310110\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.168847\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.267488\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.225720\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.424266\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.108859\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.117153\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.135963\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.103680\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.220712\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.048891\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.196195\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.152513\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.203270\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.211540\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.240247\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.133026\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.226920\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.237267\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.206298\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.149432\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.131103\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.183636\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.272522\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.190648\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.196215\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.174197\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.071748\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.400799\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.439530\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.213364\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.285200\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.093873\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.210878\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.270456\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.136735\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.201140\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.302147\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.185599\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.194007\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.107734\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.237098\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.090118\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.086172\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.147841\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.268466\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.242368\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.280305\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.124275\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.156447\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.177055\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.099952\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.153603\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.305631\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.227195\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.073432\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.099268\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.283470\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.143996\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.345945\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.160086\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.278107\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.157722\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.536230\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.408206\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.230558\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.370375\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.236107\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.237472\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.157922\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.183849\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.204183\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.090960\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.166878\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.337865\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.111745\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.369916\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.200347\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.098259\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.078213\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.319295\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.208922\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.165006\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.119344\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.086666\n",
      "\n",
      "Test set: Avg. loss: 0.0593, Accuracy: 9820/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.040617\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.093620\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.440213\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.152672\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.203350\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.245471\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.375045\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.136715\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.161457\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.194910\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.296275\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.163860\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.083392\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.254927\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.245588\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.101838\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.257013\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.075703\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.374648\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.231992\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.198876\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.371263\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.047243\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.162351\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.131785\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.225237\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.224002\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.117099\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.244888\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.268030\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.195418\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.177068\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.023084\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.124681\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.258500\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.094653\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.300317\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.423376\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.131428\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.103843\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.276415\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.186248\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.156079\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.310611\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.255567\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.315700\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.273471\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.308257\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.196502\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.074816\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.189071\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.198222\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.208212\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.277109\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.230028\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.318665\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.499225\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.115707\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.580612\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.035780\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.138678\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.298764\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.294326\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.147785\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.343116\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.157407\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.139268\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.226465\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.113089\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.328857\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.059355\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.259659\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.262954\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.354668\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.236641\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.272626\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.082425\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.253129\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.338952\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.349727\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.146285\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.270633\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.254661\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.099751\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.138486\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.079647\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.054464\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.335371\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.255365\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.113212\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.129852\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.075109\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.188987\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.246719\n",
      "\n",
      "Test set: Avg. loss: 0.0527, Accuracy: 9834/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.251737\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.109936\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.043662\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.344667\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.118224\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.131435\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.233694\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.246541\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.276237\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.202500\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.123410\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.152690\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.081620\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.354737\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.116603\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.230575\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.083966\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.135712\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.145013\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.177171\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.130967\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.326791\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.498175\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.181286\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.111239\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.192035\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.042384\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.186647\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.131009\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.233087\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.018924\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.174250\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.189096\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.144310\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.073780\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.139464\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.108747\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.127275\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.186651\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.145399\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.149200\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.117314\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.146839\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.108831\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.103772\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.262583\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.023626\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.288539\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.115840\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.233087\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.114906\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.063563\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.124846\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.188247\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.194572\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.123011\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.057375\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.085627\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.211087\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.394335\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.101835\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.109475\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.105685\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.232529\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.306839\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.297682\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.238385\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.305286\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.108955\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.284745\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.273757\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.091844\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.156186\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.230946\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.149772\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.267537\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.279359\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.157312\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.080230\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.232053\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.133496\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.087980\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.098879\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.186237\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.598576\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.412090\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.344861\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.252741\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.216226\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.240537\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.257811\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.170902\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.176652\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.297467\n",
      "\n",
      "Test set: Avg. loss: 0.0467, Accuracy: 9855/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.212038\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.022169\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.309932\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.186000\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.343224\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.045667\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.095514\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.127407\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.317526\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.070502\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.327463\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.400753\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.225031\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.170467\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.152724\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.247878\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.152179\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.059699\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.147539\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.162213\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.274856\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.146611\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.071439\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.057345\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.160039\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.265284\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.143668\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.239881\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.058566\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.467562\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.202974\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.092717\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.074738\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.074247\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.264745\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.045458\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.076162\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.392223\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.491270\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.264362\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.263237\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.211424\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.141570\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.317693\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.079804\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.160854\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.198813\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.158638\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.266736\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.224240\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.126017\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.105937\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.291849\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.427194\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.233095\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.110810\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.351692\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.220344\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.189138\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.070163\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.151404\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.124521\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.254470\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.077519\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.271034\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.097200\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.089804\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.258226\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.158006\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.261894\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.292565\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.323022\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.053916\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.211198\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.270483\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.257257\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.056489\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.397264\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.114293\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.187183\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.145635\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.210983\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.194980\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.217195\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.461983\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.500850\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.389689\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.214684\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.091401\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.111180\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.078040\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.121619\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.371585\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.334109\n",
      "\n",
      "Test set: Avg. loss: 0.0504, Accuracy: 9844/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.141716\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.224435\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.213608\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.164822\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.110911\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.053705\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.347473\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.417477\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.235579\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.043535\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.142552\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.047812\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.166281\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.304168\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.334549\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.131533\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.153464\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.206574\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.171041\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.143432\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.060129\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.239572\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.143666\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.212765\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.167461\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.249280\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.094329\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.133903\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.141577\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.054756\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.096774\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.056230\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.421568\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.050918\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.088154\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.141673\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.112715\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.098478\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.164924\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.085343\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.174754\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.226632\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.081974\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.132056\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.316238\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.096870\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.145276\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.066068\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.205140\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.168773\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.327813\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.227348\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.151950\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.186783\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.109147\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.093850\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.276853\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.198860\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.496416\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.169833\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.189147\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.339957\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.141964\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.112191\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.086675\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.040987\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.141702\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.277286\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.330818\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.195118\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.198037\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.047121\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.128756\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.057521\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.141895\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.137304\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.151986\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.182530\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.117932\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.026125\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.092543\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.315455\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.085354\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.177256\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.388699\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.146091\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.191420\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.312979\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.097266\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.111374\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.155885\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.250382\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.289147\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.129576\n",
      "\n",
      "Test set: Avg. loss: 0.0564, Accuracy: 9822/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167347e",
   "metadata": {},
   "source": [
    "# MNIST Training and Test Results Explained\n",
    "\n",
    "This document clarifies the training logs and test results from your MNIST model, helping you understand what the numbers mean and why they change during training.\n",
    "\n",
    "---\n",
    "\n",
    "## Initial Test Results (Before Training)\n",
    "\n",
    "When you first test the untrained model, you'll see something like:\n",
    "\n",
    "- **Accuracy: ~9%**\n",
    "- **Average Loss: ~2.30**\n",
    "\n",
    "**What this means:** The model starts with randomly initialized weights and essentially guesses uniformly across all 10 digit classes. Since MNIST has 10 classes (digits 0-9), random guessing yields approximately 10% accuracy. The loss of ~2.30 comes from the Negative Log-Likelihood Loss calculation: log(1/10) â‰ˆ -2.302.\n",
    "\n",
    "This baseline confirms your model is untrained and behaving as expected before any learning occurs.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Progress (Epoch 1)\n",
    "\n",
    "As training begins, you'll see the loss decrease rapidly:\n",
    "\n",
    "```\n",
    "Train Epoch: 1 [0/60000 (0%)]      Loss: 2.325928\n",
    "Train Epoch: 1 [1280/60000 (2%)]   Loss: 1.733285\n",
    "Train Epoch: 1 [2560/60000 (4%)]   Loss: 1.026400\n",
    "Train Epoch: 1 [6400/60000 (11%)]  Loss: 0.610975\n",
    "```\n",
    "\n",
    "**What's happening:** With each batch, the model performs these steps:\n",
    "\n",
    "```python\n",
    "output = network(data)\n",
    "loss = F.nll_loss(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "The network processes a batch, calculates how wrong it was, computes gradients via backpropagation, and updates its weights. The dramatic loss reductionâ€”from 2.33 to 0.61 in just the first epochâ€”shows the model rapidly learning fundamental features like edges, strokes, and basic digit shapes. This steep initial improvement is characteristic of MNIST's relative simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Test Results (After Training)\n",
    "\n",
    "After completing all training epochs:\n",
    "\n",
    "- **Accuracy: ~98%** (9,822 out of 10,000 correct)\n",
    "- **Average Loss: <0.10**\n",
    "\n",
    "**What this means:** The model has learned to generalize well to unseen data. A 98% accuracy rate demonstrates strong performance on MNIST, while the very low loss indicates the model makes confident, correct predictionsâ€”assigning high probabilities to the right digit classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
